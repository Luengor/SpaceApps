{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba9bccfb-7d33-4c6c-8343-b95f97a3ab74",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Auxiliary functions\n",
    "This file contains all the algorithms used for cleaning the signal given and removing the noise. It also includes some auxiliar functions that are implemented in the code of the algorithms or that are used in the project in general."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f5967f-3ea0-4fbf-b1a4-880fdc84cc08",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4610b6a-ba71-4243-9554-394a036adce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: netCDF4 in ./.venv/lib/python3.10/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (1.23.3)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.10/site-packages (4.64.1)\n",
      "Requirement already satisfied: numba in ./.venv/lib/python3.10/site-packages (0.56.2)\n",
      "Requirement already satisfied: cftime in ./.venv/lib/python3.10/site-packages (from netCDF4) (1.6.2)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in ./.venv/lib/python3.10/site-packages (from numba) (0.39.1)\n",
      "Requirement already satisfied: setuptools<60 in ./.venv/lib/python3.10/site-packages (from numba) (59.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install netCDF4 numpy tqdm numba netCDF4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f161268-2fab-4191-8319-9aeeea18bf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "from numba import jit, cuda\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    action=\"ignore\", category=numba.NumbaPerformanceWarning, module=\"numba\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271092e4-07ec-4216-a903-d43fa15c6e95",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Miscellaneous functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8231036a-29e8-4345-9e77-a7ef0e95802e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### read_nc(nc_filename)\n",
    "<a id=\"read_nc\"></a>\n",
    "This simple function just reads the data we want out of the files we were providen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacece75-7e29-4b70-b6be-b3da1ace86f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nc(nc_filename):\n",
    "    # We use the Dataset class from the netCDF4 library to help us open and extract data from the nc file in an easy way\n",
    "    rootgrp = Dataset(nc_filename, \"r\")\n",
    "    \n",
    "    latitude_list = rootgrp[\"science/grids/data/latitude\"][:].data\n",
    "    longitude_list = rootgrp[\"science/grids/data/longitude\"][:].data\n",
    "    \n",
    "    # We return a tuple with a list of latitudes and a list of longitudes, but if we uncomment the lines below\n",
    "    # we can instead (or also) return a tuple fo the form: (max_latitude, max_longitud, min_latitude, min_longitud),\n",
    "    # which constitutes the corners of the \"imgage\"\n",
    "    #image_corner_top_left = (np.max(latitude_list), np.min(longitude_list))\n",
    "    #image_corner_bottom_right = (np.min(latitude_list), np.max(longitude_list))\n",
    "    #image_corners = image_corner_top_left, image_corner_bottom_right\n",
    "    \n",
    "    unwrapped_phase = rootgrp[\"science/grids/data/unwrappedPhase\"][:].data\n",
    "    coherence = rootgrp[\"science/grids/data/coherence\"][:].data\n",
    "    amplitude = rootgrp[\"science/grids/data/amplitude\"][:].data\n",
    "    connected_components = rootgrp[\"science/grids/data/connectedComponents\"][:].data\n",
    "    \n",
    "    wavelength = rootgrp[\"science/radarMetaData/wavelength\"][:].data\n",
    "    \n",
    "    rootgrp.close()\n",
    "    \n",
    "    # all the variables returned are np.arrays\n",
    "    return (latitude_list, longitude_list), unwrapped_phase, coherence, amplitude, connected_components, wavelength"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a9c9c3-dce7-4a64-9b77-9fafab6aab5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### draw_phase_amplitude(unwrapped_phase, amplitude)\n",
    "<a id=\"print_phase_amplitude\"></a>\n",
    "This functions draws the wrapped phase (–π, π) on top of the amplitude, with a colorbar as a legend, returns the figure so\n",
    "we can print."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd73b6f-4f9f-4d1d-bca2-f8cfa944fab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def draw_phase_amplitude(unw_phase, amp, size=(15, 15), alpha=0.5, ticks=True, title=\"Wrapped phase on top of amplitude\"):\n",
    "    w_phase = np.fmod(unw_phase, np.pi)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_title(title)\n",
    "\n",
    "    ax.imshow(amp, cmap=\"gray\", alpha=1)\n",
    "    img = ax.imshow(w_phase, cmap=\"rainbow\", alpha=alpha)\n",
    "    \n",
    "    if ticks:\n",
    "        cbar = plt.colorbar(img, ax=ax, shrink=0.5, ticks=[3.13, 3.14/2, 0, -3.14/2, -3.13])\n",
    "        cbar.ax.set_yticklabels(['π', 'π/2', '0', '-π/2', '-π'])\n",
    "    else:\n",
    "        plt.colorbar(img, ax=ax, shrink=0.5)\n",
    "        \n",
    "    fig.set_size_inches(size[0], size[1])\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6891e4-49bb-4c74-b4e7-297a60aea270",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### rudimentary_reshape(all_imgs)\n",
    "\n",
    "This is the first functin we did to transfor all the images to the same size, it is also very crude and it doesn't work well if the difference between sizes is too big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d44f32-5b4a-4079-9b8a-1c219edd7b9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rudimentary_reshape(all_imgs):\n",
    "    # slicing the images, making them the same size (min size of all the images given)\n",
    "    r = []\n",
    "    c = []\n",
    "    for img in all_imgs:\n",
    "        r.append(img.shape[0]) # the height of the picture\n",
    "        c.append(img.shape[1]) # the width of the picture\n",
    "\n",
    "    return [img[:min(r), :min(c)] for img in coherence_imgs_arr]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0925126-e898-45ff-bd54-f0fa73bc0968",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### normalize(unwrapped_phase_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c2611a-d296-4220-9e78-10f586b8a981",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize(unwrapped_phase_mat):\n",
    "    vmin, vmax = np.min(unwrapped_phase_mat), np.max(unwrapped_phase_mat)\n",
    "    return (unwrapped_phase_mat - vmin) / (vmax - vmin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533c77f3-3b28-49d2-a987-eaebd9a8d692",
   "metadata": {
    "tags": []
   },
   "source": [
    "### reshape(list_of_images)\n",
    "\n",
    "In this part of code we get all the latitudes and logitudes of the images and reshape all them to the same size in the intersection of all the images that we are reshaping. So that we can overlap images without problems.\n",
    "It is divided in two parts: get_data(elements); and change_shape(input_).\n",
    "The first part gets the list of nc files of the data we want and the latter makes the changes of the shape of the list of images in function of the elements we've got in get_data()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7ee251-b6a5-42a7-9237-18b97ed8ef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function gets the elements Corners (used to reshape the different images to the same scale at the same point (latitude,longitude))\n",
    "And the data of unwrapped_phase and coherence (the final output we have to reshape)\n",
    "'''\n",
    "# def get_data(ncdata):\n",
    "#     unwrapped_phase_array = []\n",
    "#     coherence_array = []\n",
    "#     corner_data = []\n",
    "#     for nc in ncdata:\n",
    "#         lat_lon_lists, unwrapped_phase, coherence, _, _, _ = nc\n",
    "\n",
    "#         unwrapped_phase_array.append(unwrapped_phase)\n",
    "#         coherence_array.append(coherence)\n",
    "#         corner_data.append((lat_lon_lists, unwrapped_phase, coherence))\n",
    "        \n",
    "#     return corner_data, unwrapped_phase_array, coherence_array\n",
    "\n",
    "# def get_data(ncdata):\n",
    "#     unwrapped_phase_array = []\n",
    "#     coherence_array = []\n",
    "#     corner_data = []\n",
    "#     for nc in ncdata:\n",
    "#         lat_lon_lists, unwrapped_phase, coherence, _, _, _ = nc\n",
    "\n",
    "#         unwrapped_phase_array.append(unwrapped_phase)\n",
    "#         coherence_array.append(coherence)\n",
    "#         corner_data.append((lat_lon_lists, unwrapped_phase, coherence))\n",
    "        \n",
    "#     return corner_data, unwrapped_phase_array, coherence_array\n",
    "\n",
    "'''\n",
    "This one is the shape changer of the list of images in function of latitudes and longitudes of the data\n",
    "'''\n",
    "def reshape(input_data):\n",
    "    # input_ = [(cordinates, uw_phase, coherence), ...] where cordinates = [([latitudes], [longitudes]), ...]\n",
    "    all_lat = []\n",
    "    all_long = []\n",
    "    \n",
    "    cordinates = []\n",
    "    for cor, _, _, _, _, _ in input_data:\n",
    "        cordinates.append(cor)\n",
    "        \n",
    "    for element in cordinates:\n",
    "        all_lat.append(element[0]) #arrays of all the latitudes\n",
    "        all_long.append(element[1]) #arrays of all the longitudes\n",
    "        \n",
    "    #max lats and max longs are the list of the touple (latitude, longitude) at the top right corner\n",
    "    #same thing to min lats and min longs but in the bottom left corner\n",
    "    max_lats = []\n",
    "    min_lats = []\n",
    "    max_longs = []\n",
    "    min_longs = []\n",
    "    for lat, long in zip(all_lat, all_long):\n",
    "        max_lats.append(max(lat))\n",
    "        min_lats.append(min(lat))\n",
    "        max_longs.append(max(long))\n",
    "        min_longs.append(min(long))\n",
    "    \n",
    "    #the intersection will be the least upperbound and greatest lowerbound of the latitudes and longitudes (the intersection)\n",
    "    new_height = min(max_lats) - max(min_lats) \n",
    "    new_width = min(max_longs) - max(min_longs)\n",
    "    \n",
    "    resized_img_list = []\n",
    "    for i, value in enumerate(input_data):\n",
    "        latlon, unwrapped_phase, coherence, amplitude, cc, wl = value\n",
    "        size_y, size_x = unwrapped_phase.shape                      \n",
    "        \n",
    "        #getting the proportional new sizes x and y\n",
    "        new_y = int(size_y * new_height / (max_lats[i] - min_lats[i]))\n",
    "        new_x = int(size_x * new_width / (max_longs[i] - min_longs[i]))\n",
    "        \n",
    "        resized_img_list.append((latlon, unwrapped_phase[:new_y, :new_x], coherence[:new_y, :new_x], amplitude[:new_y, :new_x], cc[:new_y, :new_x], wl))\n",
    "    \n",
    "    return resized_img_list\n",
    "\n",
    "# def reshape(ncdata):\n",
    "#     # data = get_data(ncdata)\n",
    "#     # data = [(lat_lon_lists, unwrapped_phase, coherence, amplitude) for lat_lon_lists, unwrapped_phase, coherence, amplitude, _, _ in ncdata]\n",
    "#     return change_shape(ncdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e0c0da-bd93-4eeb-be43-30c226655ace",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acce8138-201e-4b41-a47f-444ba02d2833",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### simple_convolution(array, resolution)\n",
    "<a id=\"simple_convolution\"></a>\n",
    "\n",
    "The simple convolution function makes an average between a range of points in the image and \"softens\" it to eliminate some noise at the cost of loss of data.\n",
    "It was the first function we did and it is very crude. The resolution of the image drops substantially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32eaa62-4c57-4876-aafb-43d8ab833345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_convolution(arr, resolution):\n",
    "    shapey, shapex = arr.shape # gets the x and y values of the shape\n",
    "    new_shape = new_shapey, new_shapex = shapey-resolution*2, shapex-resolution*2 # the new shape is the shape minus the external arrays of the image\n",
    "    new_arr = np.zeros(new_shape)\n",
    "\n",
    "    for i in tqdm(range(resolution, new_shapey), desc=\"Simple convolution\"):\n",
    "        for j in range(resolution, new_shapex):\n",
    "            new_arr[i-1, j-1] = arr[i-resolution:i+resolution+1, j-resolution:j+resolution+1].sum() # this makes the average between the \"resolution\" size matrixes \n",
    "            \n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9525285-9465-4ed8-b963-1df1aaa1bb76",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### convolution_with_kernel(array, kernel)\n",
    "<a id=\"ConvKer\"></a>\n",
    "\n",
    "This was the second function of convolution we did, the difference is that there is the variable kernel, that is an array with the values for a ponderate convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71895691-50e3-4224-beed-9ed0481b1682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution_with_kernel(array, kernel):\n",
    "    kernel_size = kernel.shape[0]\n",
    "    ks = int(kernel_size / 2)\n",
    "    new_image = np.zeros(shape=(array.shape[0] - (2 * ks), array.shape[1] - (2 * ks)))\n",
    "    \n",
    "    for y in range(ks, array.shape[0] - ks):\n",
    "        for x in range(ks, array.shape[1] - ks):\n",
    "            out = np.multiply(kernel, array[y - ks:y + ks+ 1, x - ks: x + ks+ 1]) # multiply the points by the kernel values and makes the ponderate average\n",
    "            new_image[y- ks, x - ks] = np.average(out)\n",
    "    \n",
    "    return new_image "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9c12ec-6b51-4534-b1b2-3e3812771a80",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### compute_exp_kernel(kernel_size, b)\n",
    "\n",
    "It calculates the values of the kernel using the negative exponential function. This kernel uses the Frost's algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f804e4-4b0a-4be2-b1f0-137240fd421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_exp_kernel(kernel_size, b):\n",
    "    # Calculate kernel\n",
    "    kernel = np.zeros(shape=(kernel_size, kernel_size))\n",
    "    center = (kernel_size - 1) // 2\n",
    "    center = np.array([1.0, 1.0]) * center\n",
    "    for y in range(kernel_size):\n",
    "        for x in range(kernel_size):\n",
    "            dis = np.linalg.norm(center - [y, x])\n",
    "            kernel[y, x] = 2.71828 ** (-b  * abs(dis))\n",
    "    \n",
    "    kernel = kernel / np.sum(kernel)\n",
    "    return kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7b7036-81c7-41f8-92a9-9ba493555fae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### convolution_with_exp(array, kernel_size, b)\n",
    "\n",
    "In this function, the values of the kernel are ponderated by the exponential function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cac710-b50e-4c02-b180-f1e673c152ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution_with_exp(array, kernel_size, b):\n",
    "    # kernel_size must always be odd\n",
    "    kernel = compute_exp_kernel(kernel_size, b)\n",
    "    return convolution_with_kernel(array, kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bab0d7e-6f75-4157-b07f-eebaca6ff3d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### convolution_in_steroids(image, kernel, new_image)\n",
    "\n",
    "A faster implementation of the convolution function written as a cuda kernel, allowing it to running in our nvidia gpus. The compilation to a cuda kernel is done by the Numba library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05854fb3-e851-450f-b0ad-97a59f28edd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit(fastmath=True)\n",
    "def convolution_in_steroids(image, kernel, new_image):\n",
    "    # Get global positions inside the cuda grid space\n",
    "    pos = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "    \n",
    "    # Compute some variables\n",
    "    total = cuda.blockDim.x * cuda.gridDim.x \n",
    "    half_kernel = kernel.shape[0] // 2 \n",
    "    new_image_size = new_image.shape[0] * new_image.shape[1]\n",
    "    \n",
    "    # Convolve in each pixel in parallel\n",
    "    for active_pos in range(pos, new_image_size, total):\n",
    "        # Local position on the image \n",
    "        new_x, new_y = active_pos % new_image.shape[1], active_pos // new_image.shape[1]\n",
    "        old_x, old_y = new_x + half_kernel, new_y + half_kernel \n",
    "        v = 0\n",
    "\n",
    "        # Compute the product of the kernel with the image \n",
    "        for i in range(kernel.shape[0] ** 2):\n",
    "            kx, ky = i % kernel.shape[1], i // kernel.shape[1]\n",
    "            v += kernel[ky, kx] * image[old_y + ky - half_kernel, old_x + kx - half_kernel]\n",
    "\n",
    "        new_image[new_y % new_image.shape[0], new_x % new_image.shape[1]] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fab0492-7f72-4669-b98b-bce293e4129b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### convolution_in_exp_steroids(image, kernel_size, b)\n",
    "\n",
    "This function just uses the above function to simplify the process. First computes the exponential kernel and then uses the fast convolution function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e694c740-8d8d-4e78-adde-cc1eb221381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution_in_exp_steroids(image, kernel_size, b, bpg=1024, tpb=512):\n",
    "    # kernel size must always be odd\n",
    "    kernel = compute_exp_kernel(kernel_size, b)\n",
    "    k2 = 2 * (kernel_size // 2)\n",
    "    new_image = np.zeros(shape=(image.shape[0] - k2, image.shape[1] - k2), dtype=np.float32)\n",
    "    \n",
    "    convolution_in_steroids[bpg, tpb](image, kernel, new_image)\n",
    "    \n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2c29b7-f9e8-4203-ac86-a6e18b8f065d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### coherentise(phase_arr, coherence_arr, kernel_size, b)\n",
    "<a id=\"coherentise\"></a>\n",
    "\n",
    "This function makes an average of a convolution with a very localized exponential ponderated by the coherence image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5610056c-b299-451d-919d-181712ed1ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coherentise(phase_arr, coherence_arr, kernel_size=15, b=10):\n",
    "    # kernel_size must always be odd\n",
    "    coherence_mat = normalize(coherence_arr * (coherence_arr > 0.1))\n",
    "    return convolution_in_exp_steroids(np.multiply(normalize(phase_arr),coherence_mat),kernel_size,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef1b00c-0aac-41f3-982b-4170b7911cfb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### average_through_time(all_imgs)\n",
    "This function reduces the noise by making a new array of the averages of the same points in different images of the same site through the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00145da-2e9f-4295-b4b6-916e7ebdb84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_through_time(all_imgs):\n",
    "    output_img = np.zeros_like(all_imgs[0])\n",
    "    for i in tqdm(range(output_img.shape[0]), desc='Average'):\n",
    "        for j in range(output_img.shape[1]):\n",
    "            # This array stores the value of the point i,j of all the images so then\n",
    "            # we can add them and calculate the average of all the values(dividing by the number of imgaes)\n",
    "            point = [img[i][j] for img in all_imgs]\n",
    "            average = sum(point)/len(all_imgs)\n",
    "            output_img[i][j] = average \n",
    "            \n",
    "    return output_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffee92f3-9c66-46ef-bfa6-21e58816a139",
   "metadata": {
    "tags": []
   },
   "source": [
    "### average_with_convolution(phase_imgs, coherence_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff0e465-f368-4378-90ce-47c5da34f069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_with_convolution(phase_imgs, coherence_imgs, kernel_size=15, b=10):\n",
    "    new_imgs = [coherentise(phase, coherence, kernel_size, b) for phase, coherence in zip(phase_imgs, coherence_imgs)]  \n",
    "    return average_through_time(new_imgs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
